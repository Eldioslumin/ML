{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6Srt5R1li0"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "In this tutorial we will implement a linear regression model. We will also implement a function that splits the available data into a training and a testing part.\n",
        "\n",
        "## Problem Setting\n",
        "\n",
        "We will use the Boston Housing Dataset. This dataset contains information collected by the U.S Census Service concerning housing in the city of Boston in the state of Massachusetts in 1978. Our goal is to predict the median value of the houses in a particular town in the city of Boston given its attributes. Check the file ’housing_features_description.txt’ for more information on the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxSLZ7w1li1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\s'\n",
            "C:\\Users\\juand\\AppData\\Local\\Temp\\ipykernel_3092\\224319983.py:14: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "### NOTE: The boston dataset has an ethical problem. More on this can e.g. be found in the scikit documentation. ###\n",
        "### Summary: The dataset contains the proportion of black people, which was assumed that racial self-segregation had a positive impact on house prices. ###\n",
        "\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "boston_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "boston_target = raw_df.values[1::2, 2]\n",
        "\n",
        "df=pd.DataFrame(boston_data)\n",
        "df.columns=['crime_rate','res_land_zoned','industry','charles_river','nox','avg_num_rooms','prop_bf_1940','dst_emply_center','rd_highway_idx','tax_rate','stdnt_tchr_ratio','prop_blacks','low_status_pct']\n",
        "X=boston_data\n",
        "y=boston_target\n",
        "\n",
        "\n",
        "\n",
        "# Our goal is to predict the median value of the houses in a particular town in the city of Boston given its attributes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkyIMHZN1li3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     mean      std   median\n",
            "crime_rate          3.614    8.602    0.257\n",
            "res_land_zoned     11.364   23.322    0.000\n",
            "industry           11.137    6.860    9.690\n",
            "charles_river       0.069    0.254    0.000\n",
            "nox                 0.555    0.116    0.538\n",
            "avg_num_rooms       6.285    0.703    6.208\n",
            "prop_bf_1940       68.575   28.149   77.500\n",
            "dst_emply_center    3.795    2.106    3.207\n",
            "rd_highway_idx      9.549    8.707    5.000\n",
            "tax_rate          408.237  168.537  330.000\n",
            "stdnt_tchr_ratio   18.456    2.165   19.050\n",
            "prop_blacks       356.674   91.295  391.440\n",
            "low_status_pct     12.653    7.141   11.360\n"
          ]
        }
      ],
      "source": [
        "df.head(10)\n",
        "summary = pd.DataFrame({\n",
        "    'mean': df.mean(),\n",
        "    'std':  df.std(),\n",
        "    'median': df.median()\n",
        "}).round(3)\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We need to make sure all our data is there, and also check any meaningful information (what ranges are we working with and such)\n",
        "\n",
        "#  Variables in order:\n",
        "#  CRIM     per capita crime rate by town\n",
        "#  ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "#  INDUS    proportion of non-retail business acres per town\n",
        "#  CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "#  NOX      nitric oxides concentration (parts per 10 million)\n",
        "#  RM       average number of rooms per dwelling\n",
        "#  AGE      proportion of owner-occupied units built prior to 1940\n",
        "#  DIS      weighted distances to five Boston employment centres\n",
        "#  RAD      index of accessibility to radial highways\n",
        "#  TAX      full-value property-tax rate per $10,000\n",
        "#  PTRATIO  pupil-teacher ratio by town\n",
        "#  B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "#  LSTAT    % lower status of the population\n",
        "#  MEDV     Median value of owner-occupied homes in $1000's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2pPZJYnLNVj"
      },
      "source": [
        "# Note 1:\n",
        "\n",
        "Think about the ethical aspects of this dataset and machine learning in general. \n",
        "\n",
        "Can you always trust your data source? Can we use every possible information for our models?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data sources will always have some kind of bias, especially when it comes to social sciences. \n",
        "# Thinking about how data is obtained and by who is always important to understand the inherent biases and also avoid falling in the same logic flaws."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJG66FPb1li3"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Write the *split_train_test(X,y,split,seed)*, given an instance matrix $X \\in \\mathbb{R}^{N \\times D}$, labels $y \\in Y^N$, a split ratio in $[0, 1]$ and a random seed $\\in \\mathbb{Z}$. Split the dataset in $(split×100)\\%$ of the instances for training our model and the rest for testing, i.e. \n",
        "\n",
        "$$ \\left|X_{\\text{train}}\\right| = \\lceil \\text{split} \\cdot N \\rceil, \\qquad |X_{\\text{train}}| + |X_{\\text{test}}| = N. $$\n",
        "Make sure you use the given random number generator seed so we all get the same results. The function is supposed to return:\n",
        "\n",
        "- X_train, y_train: the training instances and labels;\n",
        "- X_test, y_test: the test instances and labels,\n",
        "\n",
        "in the same order as was mentioned.\n",
        "\n",
        "Hint: It may be helpful to use shuffling functionality (e.g. np.random.shuffle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ2by_fO1li4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def split_train_test(X,y,split,seed):\n",
        "    ##################\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    N = len(X)\n",
        "    index = np.arange(N)\n",
        "    np.random.shuffle(index)\n",
        "    \n",
        "    train_examples = int((split) * N)\n",
        "    test_examples = N - train_examples\n",
        "    \n",
        "    train_index = index[:train_examples]\n",
        "    test_index = index[-test_examples:]\n",
        "    \n",
        "    X_train, y_train = X[train_index], y[train_index]\n",
        "    X_test, y_test = X[test_index],y[test_index]\n",
        "    \n",
        "    ##################\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxm9S36_1li4"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Write the function *train_linear_reg(X_train,y_train,lmbd)*.\n",
        "Implement the ridge regression model (slide 24). The function should output the learned weight vector $\\theta \\in \\mathbb{R}^D$ or $\\mathbb{R}^{D+1}$ (depending on whether you are adding *bias*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "3LDp7ssz1li5"
      },
      "outputs": [],
      "source": [
        "def train_linear_reg(X, y, lmbd):\n",
        "    \n",
        "    \n",
        "    ##################\n",
        "    theta = np.dot(np.linalg.inv(np.dot(X.T, X) + (lmbd * np.eye(X.shape[1]))), np.dot(X.T, y))\n",
        "    ##################\n",
        "    return theta\n",
        "\n",
        "# To add a bias\n",
        "\n",
        "\n",
        "\n",
        "# Try to actually remember what's the deal with ridge regression.\n",
        "# So ridge regression is a combination of Squared loss + L2 regularization.\n",
        "# It's a convex optimization criterion, with only one global minimum.\n",
        "\n",
        "# Only practical for relatively small number of attributes.\n",
        "# Otherwise: use stochastic gradient method. \n",
        "# Why? Ridge regression has a heavy memory cost because of inverting matrixes. \n",
        "# SGD however uses small batches for trainig, so all good.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzAAV9et1li5"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Write the function *predict(X,theta)* which predicts housing values vector pred for a dataset X and a previously trained parameter vector $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "1k7w4Iyq1li6"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "    ##################\n",
        "    y_pred = np.dot(X, theta)\n",
        "    ##################\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H83oY9Zn1li6"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Write the function *mean_abs_loss(y_true,y_pred)* which computes the mean of the absolute differences between our prediction vector $y\\_pred$ and the real housing values $y\\_true$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "hiNjDcM-1li7"
      },
      "outputs": [],
      "source": [
        "def mean_abs_loss(y_true,y_pred):\n",
        "    ##################\n",
        "    #INSERT CODE HERE#\n",
        "    \n",
        "    absolute_loss =np.abs(y_true - y_pred)\n",
        "    mean = np.mean(absolute_loss)\n",
        "    return mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9CXkha41li7"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "Evaluate your solutions by running the following code. \n",
        "\n",
        "Moreover, answer the following questions: What is the most important feature in your model? Are there features that are not so important? What happens if you remove them? Are there outliers with a high absolute loss?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "v-hRUKyC1li8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "354 152\n",
            "The mean absolute loss is 3414.211\n",
            "22532.806324110676\n"
          ]
        }
      ],
      "source": [
        "seed = 3\n",
        "lmbd=1\n",
        "split=0.7\n",
        "X_train,y_train,X_test,y_test=split_train_test(X,y,split,seed)\n",
        "theta=train_linear_reg(X_train,y_train,lmbd)\n",
        "y_pred=predict(X_test,theta)\n",
        "mae=mean_abs_loss(y_test,y_pred)\n",
        "print ('The mean absolute loss is {loss:0.3f}'.format(loss=mae*1000))\n",
        "\n",
        "summary_y = (\n",
        "    y.mean(),\n",
        "    y.std(),\n",
        ")\n",
        "print(y.mean()*1000)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "354 152\n",
            "scikit‑learn Ridge MAE : 3.424824\n",
            "your model     MAE     : 3.414211\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- your data split ---------------------------------------------------\n",
        "seed   = 3\n",
        "lmbd   = 1          # regularization strength (alpha in scikit‑learn)\n",
        "split  = 0.7\n",
        "X_train, y_train, X_test, y_test = split_train_test(X, y, split, seed)\n",
        "\n",
        "# --- scikit‑learn Ridge regression -------------------------------------\n",
        "# If your own model adds a bias (column of ones) inside train_linear_reg,\n",
        "# keep fit_intercept=False so both use the same design matrix.\n",
        "ridge = Ridge(alpha=lmbd, fit_intercept=True, random_state=seed)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred_sklearn = ridge.predict(X_test)\n",
        "\n",
        "# --- your custom ridge implementation ----------------------------------\n",
        "theta         = train_linear_reg(X_train, y_train, lmbd)\n",
        "y_pred_custom = predict(X_test, theta)\n",
        "\n",
        "# --- compare mean‑absolute error ---------------------------------------\n",
        "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
        "mae_custom  = mean_absolute_error(y_test, y_pred_custom)\n",
        "\n",
        "print(f\"scikit‑learn Ridge MAE : {mae_sklearn:.6f}\")\n",
        "print(f\"your model     MAE     : {mae_custom :.6f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab07_LinearRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
