{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0ChBBhxF0K"
      },
      "source": [
        "# Linear Classification\n",
        "\n",
        "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
        "\n",
        "\n",
        "## Problem Setting\n",
        "\n",
        "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qSGEB3UkxF0L",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "testfile = urllib.request.URLopener()\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
        "\n",
        "df_train = pd.read_csv('SPECT.train',header=None)\n",
        "df_test = pd.read_csv('SPECT.test',header=None)\n",
        "\n",
        "train = df_train.values\n",
        "test = df_test.values\n",
        "\n",
        "y_train = train[:,0]\n",
        "X_train = train[:,1:]\n",
        "y_test = test[:,0]\n",
        "X_test = test[:,1:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#First I'd like to understand my data and the problem at hand.\n",
        "#We got a very small data set with only 87 examples.\n",
        "# All features are binary, and so is our target feature, normal or abnormal tomography.\n",
        "# Since we have our target feature in our data as a label, we are talking about supervised learning.\n",
        "# And since we only have two classes, we are in a binary classification situation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPhAtmexF0N"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
        "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$. \n",
        "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$. \n",
        "To adapt the learning rate the Barzilai-Borwein method is used.\n",
        "\n",
        "Try to understand each step of the learning algorithm and comment each line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1-HgTxIQxF0N"
      },
      "outputs": [],
      "source": [
        "def learn_reg_ERM(X,y,lbda):\n",
        "    max_iter = 200  # Maximum number of iterations\n",
        "    e  = 0.001      # Convergence threshold\n",
        "    alpha = 1.      # Initial learning rate\n",
        "\n",
        "    # Initialize random weights from normal distribution\n",
        "    w = np.random.randn(X.shape[1])\n",
        "    \n",
        "    for k in np.arange(max_iter):\n",
        "        # Compute predictions (linear combination of features and weights)\n",
        "        h = np.dot(X,w)\n",
        "        \n",
        "        # Compute loss and gradient of loss\n",
        "        l, lg = loss(h, y)\n",
        "        print('loss: {}'.format(np.mean(l)))\n",
        "        \n",
        "        # Compute regularization term and its gradient\n",
        "        r, rg = reg(w, lbda)\n",
        "        \n",
        "        # Compute total gradient (data term + regularization)\n",
        "        g = np.dot(X.T, lg) + rg \n",
        "        \n",
        "        # Barzilai-Borwein adaptive learning rate adjustment\n",
        "        if (k > 0):\n",
        "            alpha = alpha * (np.dot(g_old.T, g_old))/(np.dot((g_old - g).T, g_old)) \n",
        "        \n",
        "        # Update weights\n",
        "        w = w - alpha * g\n",
        "        \n",
        "        # Check for convergence (if weight change is small enough)\n",
        "        if (np.linalg.norm(alpha * g) < e):\n",
        "            break\n",
        "            \n",
        "        # Store current gradient for next iteration\n",
        "        g_old = g\n",
        "        \n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1CmQjDhxF0O"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient. \n",
        "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vct3IsAYxF0O"
      },
      "outputs": [],
      "source": [
        "def loss(h, y):\n",
        "    # Compute hinge loss: max(0, 1 - y*h)\n",
        "    l = np.maximum(0, 1 - y * h)\n",
        "    \n",
        "    # Compute gradient: -y if loss > 0, else 0\n",
        "    g = np.where(l > 0, -y, 0)\n",
        "    \n",
        "    return l, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmdwcNAaxF0P"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$. \n",
        "\n",
        "\n",
        "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
        "\n",
        "$$g = \\lambda \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CEsrqBTfxF0P"
      },
      "outputs": [],
      "source": [
        "def reg(w, lbda):\n",
        "    # Compute L2 regularization term: (λ/2)*w^T*w\n",
        "    r = 0.5 * lbda * np.dot(w.T, w)\n",
        "    \n",
        "    # Compute gradient: λ*w\n",
        "    g = lbda * w\n",
        "    \n",
        "    return r, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlyhFPmxF0Q"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bnKXor1NxF0Q"
      },
      "outputs": [],
      "source": [
        "def predict(w, X):\n",
        "    # Compute predictions using sign function\n",
        "    preds = np.sign(np.dot(X, w))\n",
        "    \n",
        "    # For single data point, return scalar instead of array\n",
        "    if preds.shape == (1,):\n",
        "        return preds[0]\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltgVMtXIxF0R"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "#### 5.1 \n",
        "Train a linear model on the training data and classify all 187 test instances afterwards using the function predict. \n",
        "Please note that the given class labels are in the range $\\left \\{0,1 \\right \\}$, however the learning algorithm expects a label in the range of $\\left \\{-1,1 \\right \\}$. Then, compute the accuracy of your trained linear model on both the training and the test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LqdCXWWYxF0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.310149461001628\n",
            "loss: 42.59919118729134\n",
            "loss: 15.175660966844521\n",
            "loss: 276.7214353695148\n",
            "loss: 88.69365671241931\n",
            "loss: 507.3790221979565\n",
            "loss: 159.1526646856061\n",
            "loss: 3278.283945303735\n",
            "loss: 883.8754022964546\n",
            "loss: 4280.757140659347\n",
            "loss: 1018.2329979110366\n",
            "loss: 5278.112337460503\n",
            "loss: 933.4985967737855\n",
            "loss: 3552.1231858265687\n",
            "loss: 690.4722523914891\n",
            "loss: 4938.374330339598\n",
            "loss: 980.433493802217\n",
            "loss: 4120.685422526997\n",
            "loss: 891.3360742764071\n",
            "loss: 4969.777422402064\n",
            "loss: 902.4005490348067\n",
            "loss: 3486.0497970167125\n",
            "loss: 690.3124531068664\n",
            "loss: 4926.377456887198\n",
            "loss: 978.7409527828447\n",
            "loss: 4118.794837300052\n",
            "loss: 891.526885293429\n",
            "loss: 4969.5421715634\n",
            "loss: 902.2802761935345\n",
            "loss: 3486.0514641600544\n",
            "loss: 690.3618656729195\n",
            "loss: 4926.367805758426\n",
            "loss: 978.7167805468592\n",
            "loss: 4118.801425230447\n",
            "loss: 891.5374474132532\n",
            "loss: 4969.541154256977\n",
            "loss: 902.2757941631655\n",
            "loss: 3486.0534831261693\n",
            "loss: 690.3642017733997\n",
            "loss: 4926.367605838301\n",
            "loss: 978.7156882367146\n",
            "loss: 4118.801771473451\n",
            "loss: 891.5379362548254\n",
            "loss: 4969.54111141054\n",
            "loss: 902.2755875385728\n",
            "loss: 3486.0535772223184\n",
            "loss: 690.3643097278824\n",
            "loss: 4926.367596703926\n",
            "loss: 978.7156377802143\n",
            "loss: 4118.801787487895\n",
            "loss: 891.537958840538\n",
            "loss: 4969.54110943268\n",
            "loss: 902.2755779923342\n",
            "loss: 3486.05358157007\n",
            "loss: 690.3643147155829\n",
            "loss: 4926.367596281944\n",
            "loss: 978.7156354490373\n",
            "loss: 4118.801788227797\n",
            "loss: 891.5379598840375\n",
            "loss: 4969.541109341299\n",
            "loss: 902.2755775512809\n",
            "loss: 3486.0535817709415\n",
            "loss: 690.3643149460233\n",
            "loss: 4926.367596262446\n",
            "loss: 978.7156353413326\n",
            "loss: 4118.801788261981\n",
            "loss: 891.5379599322492\n",
            "loss: 4969.541109337076\n",
            "loss: 902.2755775309033\n",
            "loss: 3486.053581780224\n",
            "loss: 690.3643149566701\n",
            "loss: 4926.367596261543\n",
            "loss: 978.7156353363562\n",
            "loss: 4118.801788263561\n",
            "loss: 891.5379599344776\n",
            "loss: 4969.54110933688\n",
            "loss: 902.2755775299622\n",
            "loss: 3486.0535817806535\n",
            "loss: 690.3643149571611\n",
            "loss: 4926.367596261503\n",
            "loss: 978.7156353361272\n",
            "loss: 4118.801788263633\n",
            "loss: 891.5379599345804\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299187\n",
            "loss: 3486.0535817806726\n",
            "loss: 690.3643149571842\n",
            "loss: 4926.367596261502\n",
            "loss: 978.7156353361152\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345852\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "loss: 978.715635336114\n",
            "loss: 4118.801788263637\n",
            "loss: 891.5379599345854\n",
            "loss: 4969.541109336873\n",
            "loss: 902.2755775299163\n",
            "loss: 3486.053581780674\n",
            "loss: 690.3643149571859\n",
            "loss: 4926.3675962615\n",
            "Training accuracy: 0.4750\n",
            "Test accuracy: 0.8877\n"
          ]
        }
      ],
      "source": [
        "# Convert labels from {0,1} to {-1,1}\n",
        "y_train = 2 * y_train - 1\n",
        "y_test = 2 * y_test - 1\n",
        "\n",
        "# Train the model\n",
        "w = learn_reg_ERM(X_train, y_train, lbda=0.01)\n",
        "\n",
        "# Make predictions\n",
        "train_preds = predict(w, X_train)\n",
        "test_preds = predict(w, X_test)\n",
        "\n",
        "# Convert predictions back to {0,1} for accuracy calculation\n",
        "train_preds = (train_preds + 1) // 2\n",
        "test_preds = (test_preds + 1) // 2\n",
        "y_train_orig = (y_train + 1) // 2\n",
        "y_test_orig = (y_test + 1) // 2\n",
        "\n",
        "# Calculate accuracies\n",
        "train_acc = np.mean(train_preds == y_train_orig)\n",
        "test_acc = np.mean(test_preds == y_test_orig)\n",
        "\n",
        "print(f\"Training accuracy: {train_acc:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdpQNg3xF0S"
      },
      "source": [
        "#### 5.2\n",
        "Compare the accuracy of the linear model with the accuracy of a random forest and a decision tree on the training and test data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l_u_jEmXxF0T"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "#INSERT CODE HERE#\n",
        "##################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab06_LinearClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
