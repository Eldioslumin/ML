{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0ChBBhxF0K"
      },
      "source": [
        "# Linear Classification\n",
        "\n",
        "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
        "\n",
        "\n",
        "## Problem Setting\n",
        "\n",
        "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qSGEB3UkxF0L",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "testfile = urllib.request.URLopener()\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
        "\n",
        "df_train = pd.read_csv('SPECT.train',header=None)\n",
        "df_test = pd.read_csv('SPECT.test',header=None)\n",
        "\n",
        "train = df_train.values\n",
        "test = df_test.values\n",
        "\n",
        "y_train = train[:,0]\n",
        "X_train = train[:,1:]\n",
        "y_test = test[:,0]\n",
        "X_test = test[:,1:]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#First I'd like to understand my data and the problem at hand.\n",
        "#We got a very small data set with only 87 examples.\n",
        "# All features are binary, and so is our target feature, normal or abnormal tomography.\n",
        "# Since we have our target feature in our data as a label, we are talking about supervised learning.\n",
        "# And since we only have two classes, we are in a binary classification situation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPhAtmexF0N"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
        "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$. \n",
        "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$. \n",
        "To adapt the learning rate the Barzilai-Borwein method is used.\n",
        "\n",
        "Try to understand each step of the learning algorithm and comment each line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1-HgTxIQxF0N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def learn_reg_ERM(X,y,lbda):\n",
        "    \n",
        "    #So we get our data matrix as X, our labels as y, and our lambda value.\n",
        "    max_iter = 200 # Max iterations would be the epochs in NLP, how many optimization runs I do.\n",
        "    e  = 0.001 # e would be epsilon, our the treshhold when we assume our model is not learning any new information.\n",
        "    alpha = 1. #Alphajor. Learning rate, our by how much we need to update our model for optimization.\n",
        "\n",
        "    w = np.random.randn(X.shape[1])\n",
        "    # So we've created a linear model that currently holds random values for all the features in our training data.\n",
        "    for k in np.arange(max_iter): #simply an array that holds values from \"0\" to \"max_iter\" to perform optimization.\n",
        "\n",
        "        h = np.dot(X,w) # This \"h\" is a dot multiplication of our features by our random model, or performed linear transformation if we want to be all nerdy about it. \n",
        "        l,lg = loss(h, y) # by using these new matrix h, we can already calculate loss and the loss gradient (how close are our initial predictions). (hinge loss I suppose?)\n",
        "        print ('loss: {}'.format(np.mean(l))) #and then we simply find the mean of the loss across all different training examples.\n",
        "        r,rg = reg(w, lbda) #I hate regularizers, me and my homies hate regularizers. It's just the bias from NLP with its ML name, so it's supposed to help the model do better predictions and avoid overfittin? right?.\n",
        "        g = np.dot(X.T,lg) + rg # We take the gradients of the loss and multiply it by a transposed X, then we add the gradient of our regularizer. All of this to update our weights. g gives us a total gradient, which will allow us to later update our model w\n",
        "        if (k > 0):# So for every iteration except the first one. \n",
        "            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old)) \n",
        "            # boi, so our learning rate is created by \n",
        "            # using itself times:\n",
        "            #   the dot product of our previous gradient descent times itself \n",
        "            #   divided by:\n",
        "            #       our the dot product of our previous gradient descent times \n",
        "            #       the previous gradient descent minus our current gradent descent (transposed).\n",
        "            # This means our learning rate is dynamically generated.\n",
        "            # Barzilai-Borwein method (research this, what alternatives are there?)\n",
        "            \n",
        "            \n",
        "        w = w - alpha * g\n",
        "        # And so, we multiply our learning rate by our gradient, and rest that amount from our model\n",
        "        if (np.linalg.norm(alpha * g) < e):\n",
        "            #If we are below our epsilon, we are free. The machine will learn no more.\n",
        "            break\n",
        "        g_old = g\n",
        "    return w\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1CmQjDhxF0O"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient. \n",
        "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Vct3IsAYxF0O"
      },
      "outputs": [],
      "source": [
        "def loss(h, y):\n",
        "    #L(y,f(x))= max(0,1−y∗f(x)) Hinge loss formula\n",
        "    #################\n",
        "    \n",
        "    l = np.maximum(0,1 - y * h)\n",
        "    g = np.zeros(l.shape)\n",
        "    g[l > 0] = -y[l > 0]\n",
        "    ##################\n",
        "    \n",
        "    return l, g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmdwcNAaxF0P"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$. \n",
        "\n",
        "\n",
        "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
        "\n",
        "$$g = \\lambda \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CEsrqBTfxF0P"
      },
      "outputs": [],
      "source": [
        "def reg(w, lbda):\n",
        "    \n",
        "    r = (lbda / 2) * np.dot(w.T, w)\n",
        "    g = lbda * w\n",
        "    return r, g\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlyhFPmxF0Q"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "bnKXor1NxF0Q"
      },
      "outputs": [],
      "source": [
        "def predict(w, X):\n",
        "    preds = np.sign(np.dot(X, w))\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltgVMtXIxF0R"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "#### 5.1 \n",
        "Train a linear model on the training data and classify all 187 test instances afterwards using the function predict. \n",
        "Please note that the given class labels are in the range $\\left \\{0,1 \\right \\}$, however the learning algorithm expects a label in the range of $\\left \\{-1,1 \\right \\}$. Then, compute the accuracy of your trained linear model on both the training and the test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "LqdCXWWYxF0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 1.460218087238283\n",
            "loss: 8.326994770489025\n",
            "loss: 4.001211163394697\n",
            "loss: 1.6013393102051647\n",
            "loss: 18.220314021922768\n",
            "loss: 6.83625156403952\n",
            "loss: 1.8367138726893006\n",
            "loss: 8.458530118235249\n",
            "loss: 3.0547791360675407\n",
            "loss: 1.9009335797641362\n",
            "loss: 0.8637790237794892\n",
            "loss: 2.1760697247382637\n",
            "loss: 0.8940503892307234\n",
            "loss: 0.8655722104499081\n",
            "loss: 0.7690936964091982\n",
            "loss: 0.6273098458012629\n",
            "loss: 0.6092341536709533\n",
            "loss: 0.6018266946550217\n",
            "loss: 0.6370639138298666\n",
            "loss: 0.5813631036752936\n",
            "loss: 0.5700328056521171\n",
            "loss: 0.5666825949882917\n",
            "loss: 0.5637865952420202\n",
            "loss: 0.561578454089591\n",
            "loss: 0.558288947231737\n",
            "loss: 0.5558114233228649\n",
            "loss: 0.55326468310815\n",
            "loss: 0.5509462980037279\n",
            "loss: 0.5492459087985878\n",
            "loss: 0.5485701328780814\n",
            "loss: 0.5474555675473487\n",
            "loss: 0.5497729841959217\n",
            "loss: 0.545326349584461\n",
            "loss: 0.5459315425068931\n",
            "loss: 0.5447606414823811\n",
            "loss: 0.5427102912614383\n",
            "loss: 0.542126698811883\n",
            "loss: 0.5424010676206519\n",
            "loss: 0.5439436660541848\n",
            "loss: 0.5405598603378976\n",
            "loss: 0.5399261925125305\n",
            "loss: 0.541155742725923\n",
            "loss: 0.5387219616752035\n",
            "loss: 0.5396343574293476\n",
            "loss: 0.538764560934291\n",
            "loss: 0.5367672260004983\n",
            "loss: 0.5367343591686307\n",
            "loss: 0.5364479694251865\n",
            "loss: 0.5360322685839655\n",
            "loss: 0.5356294427882208\n",
            "loss: 0.534557628905878\n",
            "loss: 0.552641437098852\n",
            "loss: 0.5362881496945652\n",
            "loss: 0.5323534964349974\n",
            "loss: 0.5303682958155217\n",
            "loss: 0.5323048215615233\n",
            "loss: 0.5291427265812232\n",
            "loss: 0.5276717175341699\n",
            "loss: 0.5268001424938238\n",
            "loss: 0.5342342707599463\n",
            "loss: 0.5278998486061497\n",
            "loss: 0.5253164978678367\n",
            "loss: 0.5248349802655438\n",
            "loss: 0.5242133165661534\n",
            "loss: 0.5241813774736629\n",
            "loss: 0.5279796573747026\n",
            "loss: 0.5233213578950238\n",
            "loss: 0.5224990297079\n",
            "loss: 0.5229242309166521\n",
            "loss: 0.5229292077579013\n",
            "loss: 0.5220689710004665\n",
            "loss: 0.5214219396984056\n",
            "loss: 0.5215265105709739\n",
            "loss: 0.521332158637444\n",
            "loss: 15.137500000001845\n",
            "loss: 7.593047459716166\n",
            "loss: 3.690258632867356\n",
            "loss: 22.63284583880333\n",
            "loss: 2.7953189745458737\n",
            "loss: 1.7487920405319397\n",
            "loss: 9.759026542376198\n",
            "loss: 3.1769840285197297\n",
            "loss: 1.3910589342617605\n",
            "loss: 12.878831386722197\n",
            "loss: 2.4856176909665324\n",
            "loss: 1.6873047762266584\n",
            "loss: 3.413903766868613\n",
            "loss: 2.796176414529468\n",
            "loss: 1.3690243102278261\n",
            "loss: 29.368010243208506\n",
            "loss: 3.6796599534260466\n",
            "loss: 3.319968185299613\n",
            "loss: 1.4560341730063506\n",
            "loss: 1.0367766801595404\n",
            "loss: 1.5689590708620713\n",
            "loss: 0.79762087142632\n",
            "loss: 0.7393335845973252\n",
            "loss: 0.749815332672617\n",
            "loss: 0.6466385123696969\n",
            "loss: 0.626957338940142\n",
            "loss: 0.6281106224195329\n",
            "loss: 0.6143571890090758\n",
            "loss: 0.5829690974409456\n",
            "loss: 0.5750201363049469\n",
            "loss: 0.5779507418200488\n",
            "loss: 0.5697656870877046\n",
            "loss: 0.5688673806096641\n",
            "loss: 0.5676258001409545\n",
            "loss: 0.5662477258023795\n",
            "loss: 0.5652704824432775\n",
            "loss: 0.6167681478868183\n",
            "loss: 0.5693197485775798\n",
            "loss: 0.5604030169989261\n",
            "loss: 0.554915254362046\n",
            "loss: 0.5528180084131433\n",
            "loss: 0.5507928670664466\n",
            "loss: 0.5549137051126072\n",
            "loss: 0.5618267452042895\n",
            "loss: 0.5751402994161336\n",
            "loss: 0.5453377700002611\n",
            "loss: 0.5471112079037941\n",
            "loss: 0.5419552342748791\n",
            "loss: 0.5415210937328736\n",
            "loss: 0.5394691894789732\n",
            "loss: 0.5389315512814454\n",
            "loss: 0.5386887715188949\n",
            "loss: 0.5417399777752603\n",
            "loss: 0.5423302388571104\n",
            "loss: 0.5380436008481739\n",
            "loss: 0.536307420453987\n",
            "loss: 0.535063647571276\n",
            "loss: 0.5530089328858\n",
            "loss: 0.6091326818888343\n",
            "loss: 0.5364816749035525\n",
            "loss: 0.5347595393873872\n",
            "loss: 0.536426235529713\n",
            "loss: 0.53026277472048\n",
            "loss: 0.5296031084004373\n",
            "loss: 0.5317473272049285\n",
            "loss: 0.5279677739920585\n",
            "loss: 0.5268811144229467\n",
            "loss: 0.5282593794393213\n",
            "loss: 0.5261235958728403\n",
            "loss: 0.5259826074260838\n",
            "loss: 0.5263132574167859\n",
            "loss: 0.5254435012065809\n",
            "loss: 0.5253135128080147\n",
            "loss: 0.5254218942480733\n",
            "loss: 0.525198237413447\n",
            "loss: 0.5248867244940589\n",
            "loss: 0.5247809991643104\n",
            "loss: 0.5248088538501402\n",
            "loss: 0.5246747869346043\n",
            "loss: 0.5245799368668764\n",
            "loss: 0.5245032409734034\n",
            "loss: 0.5280035322751602\n",
            "loss: 0.5241793308914268\n",
            "loss: 0.5238877516187342\n",
            "loss: 0.5246598440482128\n",
            "loss: 0.5236747729915538\n",
            "loss: 0.5234082065116171\n",
            "loss: 0.5232402481580589\n",
            "loss: 0.523204449442474\n",
            "loss: 0.5232950691640653\n",
            "loss: 0.5234327282412546\n",
            "loss: 0.5229396037865827\n",
            "loss: 0.522957468808509\n",
            "loss: 0.5228719176402521\n",
            "loss: 0.5227651229934979\n",
            "loss: 0.5239545004239881\n",
            "loss: 0.5226321808303245\n",
            "loss: 0.5224995958853199\n",
            "loss: 0.523195529878742\n",
            "loss: 0.5234662855247817\n",
            "loss: 0.5221330724549305\n",
            "loss: 0.5221506084793187\n",
            "loss: 0.522455732052154\n",
            "loss: 0.5220632207221938\n",
            "loss: 0.5220957861798953\n",
            "loss: 0.5219891136748089\n",
            "loss: 0.5219370945865047\n",
            "loss: 0.5218722269809518\n",
            "loss: 0.521862218425026\n",
            "loss: 0.5217784672462494\n",
            "loss: 0.5218523415422587\n",
            "loss: 0.5217326692812577\n",
            "loss: 0.52171474092535\n",
            "loss: 0.5216484901213339\n",
            "loss: 0.5222121169396173\n",
            "loss: 0.5218495410027467\n",
            "loss: 0.5215690490501659\n",
            "loss: 0.5214701170946698\n",
            "loss: 0.5213821734637799\n",
            "loss: 0.5213193886397653\n",
            "loss: 0.5221308518026714\n",
            "loss: 0.5211960598514109\n",
            "loss: 0.5210865614150796\n",
            "loss: 0.5211436015819537\n",
            "loss: 0.5220310353308859\n",
            "loss: 0.5208743760736002\n",
            "0.6898395721925134\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "##################\n",
        "y_train[y_train == 0] = -1\n",
        "y_test[y_test == 0] = -1\n",
        "\n",
        "w = learn_reg_ERM(X_train, y_train, 0.1)\n",
        "y_hat = predict(w, X_test)\n",
        "accuracy = np.mean(y_test == y_hat)\n",
        "print(accuracy)\n",
        "##################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdpQNg3xF0S"
      },
      "source": [
        "#### 5.2\n",
        "Compare the accuracy of the linear model with the accuracy of a random forest and a decision tree on the training and test data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l_u_jEmXxF0T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: OrderedDict({'max_depth': 27, 'max_features': 0.8472525481451924, 'min_samples_leaf': 1, 'min_samples_split': 20})\n",
            "Best score: 0.775\n",
            "Training Accuracy: 0.9375\n",
            "Test Accuracy: 0.7861\n"
          ]
        }
      ],
      "source": [
        "##################\n",
        "#INSERT CODE HERE#\n",
        "np.random.seed(404)\n",
        "\n",
        "clf_tree = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "search_space = {\n",
        "    'max_depth': (1, 50),                        \n",
        "    'min_samples_split': (2, 20),                 \n",
        "    'min_samples_leaf': (1, 20),                  \n",
        "    'max_features': (0.1, 1.0, 'uniform'),      \n",
        "}\n",
        "\n",
        "opt = BayesSearchCV(\n",
        "    clf_tree,\n",
        "    search_spaces=search_space,\n",
        "    n_iter=32,                \n",
        "    scoring='accuracy',       \n",
        "    cv=5, # cv=5 means each combination is tested with 5‑fold cross‑validation (done in parallel via n_jobs=-1).         \n",
        "    random_state=42,\n",
        "    n_jobs=-1                 \n",
        ")\n",
        "\n",
        "opt.fit(X_train, y_train)\n",
        "print(\"Best parameters:\", opt.best_params_)\n",
        "print(\"Best score:\", opt.best_score_)\n",
        "\n",
        "##################\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "train_accuracy = clf.score(X_train, y_train)\n",
        "test_accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab06_LinearClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
