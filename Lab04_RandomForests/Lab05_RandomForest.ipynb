{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "In this lab you will learn the most important aspects of the random forest learning method. \n",
    "Completing this lab and analyzing the code will give you a deeper understanding of these type of models.\n",
    "In our experiments we will mostly use the package sklearn from which we import RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation\n",
    "\n",
    "First of all, we create a data set containing 1000 samples with 2 features and two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 1000,n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1:</b>\n",
    "\n",
    "Visualize the data set. It should look like this:\n",
    "<img src=\"figures/dataset.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.DataFrame(X, columns=['feature1', 'feature2'])\n",
    "y2 = pd.DataFrame(y, columns=['target'])\n",
    "mydata = pd.concat([X2, y2], axis=1)\n",
    "scatter = plt.scatter(mydata['feature1'], mydata['feature2'], c=mydata['target'], cmap='bwr')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.legend(handles, labels, title='target', loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2:</b>\n",
    "\n",
    "We split our data into train and test data. Then we can train our model (a random forest) on the train data and evaluate the model on the hold out test data. We split the data in a way that we train our model on 67% of the data and test our model on 33% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a random forest on the training data and report the accuracy for this model on the train and test data using the default parameters of a random forest (from sklearn). What can you conclude from this? ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. fit(X, y)\n",
    "# Trains the random forest model on the dataset X with labels y.\n",
    "# Each tree in the forest is trained on a bootstrap sample of the data.\n",
    "# Example:\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# 2. predict(X)\n",
    "# Predicts class labels for the samples in X by aggregating the predictions from all individual trees (majority voting).\n",
    "# Example:\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# 3. predict_proba(X)\n",
    "# Returns the class probabilities for the samples in X.\n",
    "# It averages the probabilities predicted by each tree.\n",
    "# Example:\n",
    "# y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "# 4. predict_log_proba(X)\n",
    "# Similar to predict_proba, but returns the logarithm of the class probabilities.\n",
    "# Example:\n",
    "# y_log_proba = clf.predict_log_proba(X_test)\n",
    "\n",
    "# 5. score(X, y)\n",
    "# Computes the mean accuracy of the model on the given test data and labels.\n",
    "# Example:\n",
    "# accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "# 6. apply(X)\n",
    "# For each sample in X, returns the index of the leaf node in each tree where the sample ends up.\n",
    "# Useful for understanding the model's decisions or for advanced tasks like proximity analysis.\n",
    "# Example:\n",
    "# leaf_indices = clf.apply(X_test)\n",
    "\n",
    "# 7. decision_path(X)\n",
    "# Provides the decision path for each sample in X through each tree in the forest.\n",
    "# Returns a sparse matrix indicating the nodes each sample passes through.\n",
    "# Helpful for model introspection and understanding how decisions are made.\n",
    "# Example:\n",
    "# decision_paths = clf.decision_path(X_test)\n",
    "\n",
    "# 8. feature_importances_\n",
    "# An array of shape (n_features,) indicating the importance of each feature in the model.\n",
    "# Importance is computed based on the (normalized) total reduction of the criterion (e.g., Gini impurity) brought by that feature.\n",
    "# Example:\n",
    "# importances = clf.feature_importances_\n",
    "\n",
    "# 9. estimators_samples_\n",
    "# A list where each element is an array of indices representing the samples used to train each individual tree.\n",
    "# Relevant when bootstrap=True, as each tree is trained on a bootstrap sample of the data.\n",
    "# Example:\n",
    "# samples_per_tree = clf.estimators_samples_\n",
    "\n",
    "# 10. get_params(deep=True)\n",
    "# Returns a dictionary of the model's parameters.\n",
    "# If deep=True, it will also return the parameters of nested estimators.\n",
    "# Example:\n",
    "# params = clf.get_params()\n",
    "\n",
    "# 11. set_params(**params)\n",
    "# Sets the parameters of the model.\n",
    "# Useful for hyperparameter tuning, especially in conjunction with tools like GridSearchCV.\n",
    "# Example:\n",
    "# clf.set_params(n_estimators=200, max_depth=10)\n",
    "\n",
    "# 12. get_metadata_routing()\n",
    "# Retrieves the metadata routing configuration of the estimator.\n",
    "# Part of scikit-learn's advanced API for handling additional data passed to estimators.\n",
    "# Example:\n",
    "# routing = clf.get_metadata_routing()\n",
    "\n",
    "# 13. set_fit_request()\n",
    "# Specifies additional metadata that should be passed to the fit method.\n",
    "# Part of scikit-learn's advanced API, typically used in complex pipelines or when integrating with other tools.\n",
    "# Example:\n",
    "# clf.set_fit_request(sample_weight=True)\n",
    "\n",
    "# 14. set_score_request()\n",
    "# Specifies additional metadata that should be passed to the score method.\n",
    "# Part of scikit-learn's advanced API, typically used in complex pipelines or when integrating with other tools.\n",
    "# Example:\n",
    "# clf.set_score_request(sample_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test.values.ravel()))\n",
    "\n",
    "# \"\"\"\n",
    "# The parameters of a random forest classifier.\n",
    "\n",
    "# (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "\n",
    "# min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "\n",
    "# bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "\n",
    "# ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "# \"\"\"\n",
    "\n",
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "#Given our results, \n",
    "\n",
    "#Also, all this problems are because of DF vs numpy stuff. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "Sometimes it is helpful to plot the decision boundary for a learned model. To do so, we create a grid of data points and calculate the probability of belonging to class 1. \n",
    "(This can only be done once we have trained the random forest classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.to_numpy()\n",
    "x_min, x_max = X.iloc[:, 0].min(), X.iloc[:, 0].max()\n",
    "y_min, y_max = X.iloc[:, 1].min(), X.iloc[:, 1].max()\n",
    "\n",
    "# I seem to be having some problems because I changed my data to a pandas dataframe.\n",
    "# x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "# y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "h = .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "#Again, problems between df and numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can plot the boundary using the 'contourf' function of matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = y.to_numpy().ravel()\n",
    "# The way i found to fix the error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.RdBu  # color map\n",
    "plt.contourf(xx, yy, Z, alpha=.8, cmap=cm)\n",
    "colors = ['red','blue']\n",
    "\n",
    "for cur_class in [0,1]:\n",
    "    plt.scatter(X[y==cur_class, 0], X[y == cur_class, 1], c=colors[cur_class],\n",
    "                       edgecolors='k', alpha=0.6, label=cur_class)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude from the figure above?\n",
    "\n",
    "Basically there are very clear sections of our graph where one class is prevalent over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Selection\n",
    "\n",
    "The implementation of the random forest algorithm in sklearn has many parameter. The most important ones are the number of trees used (n_estimators) and the maximal depth of a single tree (max_depth). Investigate how the number of used trees effects the training and testing accuracy.\n",
    "\n",
    "<b>Exercise 3:</b>\n",
    "\n",
    "Plot a diagram that shows the training and testing accuracy depending on the number of trees (from 1 to 20) used. This plot should look like this:\n",
    "<img src=\"figures/num_trees.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "# (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "\n",
    "# min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "\n",
    "# bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, \n",
    "\n",
    "# ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "# \"\"\"\n",
    "\n",
    "y_train_flat = y_train.to_numpy().ravel()\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for i in range(1, 21):\n",
    "    clf = RandomForestClassifier(n_estimators=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    a = clf.score(X_train,y_train)\n",
    "    b = clf.score(X_test,y_test)\n",
    "\n",
    "    train_acc.append(a)\n",
    "    test_acc.append(b)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 21), train_acc, label='Train Accuracy')\n",
    "plt.plot(range(1, 21), test_acc, label='Test Accuracy')\n",
    "    \n",
    "\n",
    "#Welp this is performing a bit better than expected.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional:</b>\n",
    "\n",
    "We want to further investigate how the number of used trees effects the training and testing accuracy. \n",
    "We want to increase the number of trees to a maximum of 50.\n",
    "\n",
    "Plot a diagram that shows the training and testing accuracy depending on the number of trees (from 1 to 50) used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for i in range(1, 51):\n",
    "    clf = RandomForestClassifier(n_estimators=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    a = clf.score(X_train,y_train)\n",
    "    b = clf.score(X_test,y_test)\n",
    "\n",
    "    train_acc.append(a)\n",
    "    test_acc.append(b)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 51), train_acc, label='Train Accuracy')\n",
    "plt.plot(range(1, 51), test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Classifier Accuracy vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4:</b>\n",
    "\n",
    "We saw how the number of trees influences training and testing accuracy. \n",
    "Now we do same thing for the other important parameter max_depth.\n",
    "Train multiple models with different depths. \n",
    "The models should use 30 trees as estimators.\n",
    "\n",
    "Plot a diagram that shows the training and testing accuracy depending on the maximal depth of a single tree (from 1 to 50). \n",
    "This plot should look like this:\n",
    "<img src=\"figures/max_depth.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for i in range(1, 51):\n",
    "    clf = RandomForestClassifier(n_estimators=30, max_depth=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    a = clf.score(X_train,y_train)\n",
    "    b = clf.score(X_test,y_test)\n",
    "\n",
    "    train_acc.append(a)\n",
    "    test_acc.append(b)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 51), train_acc, label='Train Accuracy')\n",
    "plt.plot(range(1, 51), test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest Classifier Accuracy vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Data Set\n",
    "Lets revisit the churn data set from the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('telecom_churn.csv')\n",
    "label = churn_df['Churn']\n",
    "churn_df = churn_df.drop(columns=['Churn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 5:</b>\n",
    "\n",
    "Create a data set containing only the numeric values. <b>Optional:</b> Try to convert all non numeric values to numeric values using a one hot encoding or by binning them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "churn_numeric = churn_df.select_dtypes(include=[np.number])\n",
    "\n",
    "nonnumeric = ['State', 'International plan', 'Voice mail plan']\n",
    "allnumeric= pd.get_dummies(churn_df, columns=nonnumeric)\n",
    "allnumeric= df_encoded.astype(np.int64)\n",
    "allnumeric\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 6:</b>\n",
    "\n",
    "Train a model on this data set and visualize the most important features in a figure. This should look like this (The scaling and order of features can be different):\n",
    "<img src=\"figures/importance.png\" width=\"600\"/>\n",
    "\n",
    "<b>Hint</b>: The method feature_importance_ should be used.\n",
    "What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(churn_numeric, label)\n",
    "a = clf.feature_importances_\n",
    "#plt.barh(churn_numeric.columns, a, color='blue')\n",
    "\n",
    "\n",
    "#Stuff I did chatgpt do cause i got annoyed. \n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=churn_numeric.columns)\n",
    "\n",
    "# Sort in descending order\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances_sorted.plot(kind='barh', color='blue')\n",
    "plt.gca().invert_yaxis()  # Most important at the top\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Sorted Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 7:</b>\n",
    "\n",
    "If we want to use a random forest to solve regression problems we can use the RandomForestRegressor from sklearn.\n",
    "* Generate an easy regression data set using make_regression with 10 features. (use function make_regression)\n",
    "* Split the data set into a train and test set.\n",
    "* Train a model and report the training and testing mean square error (can be calculated using sklearn.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1973.7351396472143\n",
      "Test MSE: 14459.979956063778\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE ###\n",
    "\n",
    "#Let's see, is this the same as the other one but with regression?\n",
    "# Lets use the most generic stuff lol\n",
    "\n",
    "X, y = make_regression(n_features=10)\n",
    "\n",
    "#(n_samples=100, n_features=100, *, \n",
    "# n_informative=10, n_targets=1, bias=0.0, effective_rank=None,\n",
    "#  tail_strength=0.5, noise=0.0, shuffle=True, coef=False,\n",
    "#  random_state=None)\n",
    "# So, way more features than last time. \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)\n",
    "#Split pretty simple i hope.\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#I cant use the score unfortunately cause it calculates something called r2, which measures how well the model explains the variance in the data.\n",
    "# so predict it is\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_error(y_train, clf.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Test MSE: {test_mse}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
